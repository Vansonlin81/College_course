# -*- coding: utf-8 -*-
"""3. 使用 Bert 預訓練模型

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B3d4XbUmm_YDWUuHgDTWUvUO30u_YGbW

# 範例程式 -- 使用　BERT　預訓練模型
"""

!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
import torch
from transformers import BertTokenizer, BertModel

import matplotlib.pyplot as plt
# %matplotlib inline

USE_PRETRAINED = 'bert-base-uncased'
# 有這些 pretrained 可以用用看
#'bert-base-chinese'        #'hfl/chinese-roberta-wwm-ext-large'    #'bert-base-uncased'

# 研究取出 word embeddings
# functions
def bert_text_preparation(text, tokenizer):
    """Preparing the input for BERT

    Takes a string argument and performs
    pre-processing like adding special tokens,
    tokenization, tokens to ids, and tokens to
    segment ids. All tokens are mapped to seg-
    ment id = 1.

    Args:
        text (str): Text to be converted
        tokenizer (obj): Tokenizer object
            to convert text into BERT-re-
            adable tokens and ids

    Returns:
        list: List of BERT-readable tokens
        obj: Torch tensor with token ids
        obj: Torch tensor segment ids


    """
    marked_text = "[CLS] " + text + " [SEP]"
    tokenized_text = tokenizer.tokenize(marked_text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    segments_ids = [1]*len(indexed_tokens)

    # Convert inputs to PyTorch tensors
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    return tokenized_text, tokens_tensor, segments_tensors

def get_bert_embeddings(tokens_tensor, segments_tensors, model):
    """Get embeddings from an embedding model

    Args:
        tokens_tensor (obj): Torch tensor size [n_tokens]
            with token ids for each token in text
        segments_tensors (obj): Torch tensor size [n_tokens]
            with segment ids for each token in text
        model (obj): Embedding model to generate embeddings
            from token and segment ids

    Returns:
        list: List of list of floats of size
            [n_tokens, n_embedding_dimensions]
            containing embeddings for each token

    """

    model.eval()

    # Run the text through BERT, and collect all of the hidden states produced
    # from all 12 layers.
    with torch.no_grad():
        outputs = model(tokens_tensor, segments_tensors)

        # Evaluating the model will return a different number of objects based on
        # how it's  configured in the `from_pretrained` call earlier. In this case,
        # becase we set `output_hidden_states = True`, the third item will be the
        # hidden states from all layers. See the documentation for more details:
        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel
        hidden_states = outputs[2]

        # Concatenate the tensors for all layers. We use `stack` here to
        # create a new dimension in the tensor.
        token_embeddings = torch.stack(hidden_states, dim=0)

        #print(token_embeddings.size())
        # Remove dimension 1, the "batches".
        token_embeddings = torch.squeeze(token_embeddings, dim=1)

        #print(token_embeddings.size())

        # Swap dimensions 0 and 1.
        token_embeddings = token_embeddings.permute(1,0,2)

        print(token_embeddings.size())
        return token_embeddings

from scipy.spatial.distance import cosine
help(cosine)

"""## 1) 嘗試英文"""

# Load pre-trained model (weights)
# Whether the model returns all hidden-states.

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained(USE_PRETRAINED)
model = BertModel.from_pretrained(USE_PRETRAINED, output_hidden_states = True )
#print(model)

# 以下面這些字詞輸入，會看到什麼
# Define a new example sentence with multiple meanings of the word "bank"
text = "After stealing money from the bank vault, the bank robber was seen " \
       "fishing on the Mississippi river bank."

# Add the special tokens.
marked_text = "[CLS] " + text + " [SEP]"

# Split the sentence into tokens.
tokenized_text = tokenizer.tokenize(marked_text)

# Map the token strings to their vocabulary indeces.
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

# Display the words with their indeces.
for tup in zip(tokenized_text, indexed_tokens):
    print('{:<12} {:>6,}'.format(tup[0], tup[1]))

# Mark each of the 22 tokens as belonging to sentence "1".
segments_ids = [1] * len(tokenized_text)

print (segments_ids)

# Convert inputs to PyTorch tensors
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])

# using get_bert_embeddings()
the_emb = get_bert_embeddings(tokens_tensor, segments_tensors, model)

print('First 5 vector values for each instance of "bank".')
print('')
print("bank vault   ", str(the_emb[6][:5]))
print("bank robber  ", str(the_emb[10][:5]))
print("river bank   ", str(the_emb[19][:5]))

from scipy.spatial.distance import cosine

# Stores the token vectors, with shape [22 x 768]
token_vecs_sum = []

# `token_embeddings` is a [22 x 12 x 768] tensor.

# For each token in the sentence...
for token in the_emb:

    # `token` is a [12 x 768] tensor

    # Sum the vectors from the last four layers.
    sum_vec = torch.sum(token[-4:], dim=0)

    # Use `sum_vec` to represent `token`.
    token_vecs_sum.append(sum_vec)

print(token_vecs_sum[6].size())

# Calculate the cosine similarity between the word bank
# in "bank robber" vs "river bank" (different meanings).
diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])

# Calculate the cosine similarity between the word bank
# in "bank robber" vs "bank vault" (same meaning).
same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])

# index of "bank" vault : 6     --> 金庫
# index of "bank" robber  : 10  --> 搶匪
# index of "river bank" : 19    --> 河岸

print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)
print('Vector similarity for *different* meanings:  %.2f' % diff_bank)

"""## 2) 嘗試中文"""

# Load pre-trained model (weights)
# Whether the model returns all hidden-states.

USE_PRETRAINED = "bert-base-chinese"
# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained(USE_PRETRAINED)
model = BertModel.from_pretrained(USE_PRETRAINED, output_hidden_states = True )
#print(model)

# 以下面這些字詞輸入，會看到什麼
# Define a new example sentence with multiple meanings of the word "bank"
str_text_chinese = "這傢伙真愛現，愛約不約，不知道有誰愛他。"


# Add the special tokens.
marked_text = "[CLS] " + str_text_chinese + " [SEP]"

# Split the sentence into tokens.
tokenized_text = tokenizer.tokenize(marked_text)

# Map the token strings to their vocabulary indeces.
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

# Display the words with their indeces.
for tup in zip(tokenized_text, indexed_tokens):
    print('{:<12} {:>6,}'.format(tup[0], tup[1]))

# Mark each of the 22 tokens as belonging to sentence "1".
segments_ids = [1] * len(tokenized_text)

print (segments_ids)

# Convert inputs to PyTorch tensors
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])

# using get_bert_embeddings()
the_emb = get_bert_embeddings(tokens_tensor, segments_tensors, model)

print('First 5 vector values for each instance of "愛".')
print('')
print("愛現   ", str(the_emb[5][:5]))
print("愛約  ", str(the_emb[8][:5]))
print("愛他   ", str(the_emb[18][:5]))

from scipy.spatial.distance import cosine

# Stores the token vectors, with shape [22 x 768]
token_vecs_sum = []

# `token_embeddings` is a [22 x 12 x 768] tensor.

# For each token in the sentence...
for token in the_emb:

    # `token` is a [12 x 768] tensor

    # Sum the vectors from the last four layers.
    sum_vec = torch.sum(token[-4:], dim=0)

    # Use `sum_vec` to represent `token`.
    token_vecs_sum.append(sum_vec)


# Calculate the cosine similarity between the word bank
# in "bank robber" vs "river bank" (different meanings).
diff_bank = 1 - cosine(token_vecs_sum[8], token_vecs_sum[5])

# Calculate the cosine similarity between the word bank
# in "bank robber" vs "bank vault" (same meaning).
same_bank = 1 - cosine(token_vecs_sum[18], token_vecs_sum[5])

# index of 愛現 : 5
# index of 愛約 : 8
# index of 愛他 : 18

print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)
print('Vector similarity for *different* meanings:  %.2f' % diff_bank)

